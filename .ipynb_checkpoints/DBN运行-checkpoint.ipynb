{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cba3d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,recall_score,confusion_matrix,f1_score,precision_score\n",
    "from sklearn.metrics import cohen_kappa_score,hamming_loss,precision_recall_curve\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from matplotlib import pyplot as plt\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "import  tensorflow\n",
    "import  pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5883dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "# digits = load_digits()\n",
    "# X, Y = digits.data, digits.target\n",
    "data = pd.read_csv(r'D:\\python\\TensorflowFiles\\data\\地质灾害易发性\\train.csv',sep=',')\n",
    "data_= pd.read_csv(r'D:\\python\\TensorflowFiles\\data\\地质灾害易发性\\test.csv',sep=',')\n",
    "#导入数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa589945",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 6\n",
    "dim  = 6\n",
    "# data[label_index] = data[label_index].map({1:1, 2:-1})\n",
    "data = data.sample(frac=1.0)\n",
    "data = data.reset_index()\n",
    "x = data.drop('landslide',axis=1)\n",
    "x = x.drop(['index'],axis=1)\n",
    "x = np.array(x)\n",
    "std=MinMaxScaler((0,1))\n",
    "x=std.fit_transform(x)\n",
    "y = data['landslide']\n",
    "y = pd.to_numeric( y, errors='coerce').fillna('0').astype('int32')\n",
    "# Data scaling\n",
    "# X = (X / 16).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5d9cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9946706, 9)\n",
      "(9946706, 8)\n"
     ]
    }
   ],
   "source": [
    "data_ = data_.sample(frac=1.0)\n",
    "data_ = data_.reset_index()\n",
    "x_ = data_.drop('landslide',axis=1)\n",
    "print(x_.shape)\n",
    "x_ = x_.drop(['index'],axis=1)\n",
    "x_ = np.array(x_)\n",
    "print(x_.shape)\n",
    "std=MinMaxScaler((0,1))\n",
    "x_=std.fit_transform(x_)\n",
    "y_ = data_['landslide']\n",
    "y_ = pd.to_numeric( y_, errors='coerce').fillna('0').astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a42c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=0)#random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f94d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.291082\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.247310\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.197739\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.172918\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.147440\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.140545\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.133736\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.123282\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.117652\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.133626\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.119313\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.116215\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.112505\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.097659\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.115044\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.103679\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.096348\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.104544\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.088786\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.094823\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.090342\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.106300\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.093044\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.077504\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.089174\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.099414\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.094539\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.110635\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.103078\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.093596\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.281008\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.288702\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.274801\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.276830\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.270586\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.263786\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.247970\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.262070\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.207372\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.189055\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.182393\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.166420\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.161240\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.152235\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.143618\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.144337\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.135246\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.135917\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.150489\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.137154\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 0.138673\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 0.132710\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 0.124245\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 0.134533\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 0.119805\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 0.117749\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 0.114934\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 0.118445\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 0.112663\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 0.115811\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.690338\n",
      ">> Epoch 1 finished \tANN training loss 0.682187\n",
      ">> Epoch 2 finished \tANN training loss 0.676090\n",
      ">> Epoch 3 finished \tANN training loss 0.670805\n",
      ">> Epoch 4 finished \tANN training loss 0.663150\n",
      ">> Epoch 5 finished \tANN training loss 0.654998\n",
      ">> Epoch 6 finished \tANN training loss 0.649152\n",
      ">> Epoch 7 finished \tANN training loss 0.644236\n",
      ">> Epoch 8 finished \tANN training loss 0.630557\n",
      ">> Epoch 9 finished \tANN training loss 0.622442\n",
      ">> Epoch 10 finished \tANN training loss 0.614657\n",
      ">> Epoch 11 finished \tANN training loss 0.617638\n",
      ">> Epoch 12 finished \tANN training loss 0.605271\n",
      ">> Epoch 13 finished \tANN training loss 0.600327\n",
      ">> Epoch 14 finished \tANN training loss 0.593293\n",
      ">> Epoch 15 finished \tANN training loss 0.593536\n",
      ">> Epoch 16 finished \tANN training loss 0.586447\n",
      ">> Epoch 17 finished \tANN training loss 0.591805\n",
      ">> Epoch 18 finished \tANN training loss 0.581483\n",
      ">> Epoch 19 finished \tANN training loss 0.613006\n",
      ">> Epoch 20 finished \tANN training loss 0.579464\n",
      ">> Epoch 21 finished \tANN training loss 0.576778\n",
      ">> Epoch 22 finished \tANN training loss 0.574289\n",
      ">> Epoch 23 finished \tANN training loss 0.588896\n",
      ">> Epoch 24 finished \tANN training loss 0.584295\n",
      ">> Epoch 25 finished \tANN training loss 0.569461\n",
      ">> Epoch 26 finished \tANN training loss 0.570646\n",
      ">> Epoch 27 finished \tANN training loss 0.586556\n",
      ">> Epoch 28 finished \tANN training loss 0.578123\n",
      ">> Epoch 29 finished \tANN training loss 0.558377\n",
      ">> Epoch 30 finished \tANN training loss 0.556526\n",
      ">> Epoch 31 finished \tANN training loss 0.594669\n",
      ">> Epoch 32 finished \tANN training loss 0.610310\n",
      ">> Epoch 33 finished \tANN training loss 0.574151\n",
      ">> Epoch 34 finished \tANN training loss 0.576618\n",
      ">> Epoch 35 finished \tANN training loss 0.552130\n",
      ">> Epoch 36 finished \tANN training loss 0.576335\n",
      ">> Epoch 37 finished \tANN training loss 0.534303\n",
      ">> Epoch 38 finished \tANN training loss 0.532117\n",
      ">> Epoch 39 finished \tANN training loss 0.563076\n",
      ">> Epoch 40 finished \tANN training loss 0.524868\n",
      ">> Epoch 41 finished \tANN training loss 0.546919\n",
      ">> Epoch 42 finished \tANN training loss 0.526468\n",
      ">> Epoch 43 finished \tANN training loss 0.518357\n",
      ">> Epoch 44 finished \tANN training loss 0.566998\n",
      ">> Epoch 45 finished \tANN training loss 0.571518\n",
      ">> Epoch 46 finished \tANN training loss 0.506226\n",
      ">> Epoch 47 finished \tANN training loss 0.505220\n",
      ">> Epoch 48 finished \tANN training loss 0.507436\n",
      ">> Epoch 49 finished \tANN training loss 0.547623\n",
      ">> Epoch 50 finished \tANN training loss 0.496588\n",
      ">> Epoch 51 finished \tANN training loss 0.503408\n",
      ">> Epoch 52 finished \tANN training loss 0.551070\n",
      ">> Epoch 53 finished \tANN training loss 0.587245\n",
      ">> Epoch 54 finished \tANN training loss 0.612554\n",
      ">> Epoch 55 finished \tANN training loss 0.492479\n",
      ">> Epoch 56 finished \tANN training loss 0.504591\n",
      ">> Epoch 57 finished \tANN training loss 0.504985\n",
      ">> Epoch 58 finished \tANN training loss 0.572847\n",
      ">> Epoch 59 finished \tANN training loss 0.512296\n",
      ">> Epoch 60 finished \tANN training loss 0.521031\n",
      ">> Epoch 61 finished \tANN training loss 0.579408\n",
      ">> Epoch 62 finished \tANN training loss 0.502704\n",
      ">> Epoch 63 finished \tANN training loss 0.501253\n",
      ">> Epoch 64 finished \tANN training loss 0.551730\n",
      ">> Epoch 65 finished \tANN training loss 0.485946\n",
      ">> Epoch 66 finished \tANN training loss 0.505219\n",
      ">> Epoch 67 finished \tANN training loss 0.508111\n",
      ">> Epoch 68 finished \tANN training loss 0.525319\n",
      ">> Epoch 69 finished \tANN training loss 0.486153\n",
      ">> Epoch 70 finished \tANN training loss 0.479323\n",
      ">> Epoch 71 finished \tANN training loss 0.482268\n",
      ">> Epoch 72 finished \tANN training loss 0.495854\n",
      ">> Epoch 73 finished \tANN training loss 0.491436\n",
      ">> Epoch 74 finished \tANN training loss 0.568451\n",
      ">> Epoch 75 finished \tANN training loss 0.490096\n",
      ">> Epoch 76 finished \tANN training loss 0.498743\n",
      ">> Epoch 77 finished \tANN training loss 0.516172\n",
      ">> Epoch 78 finished \tANN training loss 0.497528\n",
      ">> Epoch 79 finished \tANN training loss 0.474913\n",
      ">> Epoch 80 finished \tANN training loss 0.494445\n",
      ">> Epoch 81 finished \tANN training loss 0.602153\n",
      ">> Epoch 82 finished \tANN training loss 0.491146\n",
      ">> Epoch 83 finished \tANN training loss 0.499852\n",
      ">> Epoch 84 finished \tANN training loss 0.475949\n",
      ">> Epoch 85 finished \tANN training loss 0.626434\n",
      ">> Epoch 86 finished \tANN training loss 0.474054\n",
      ">> Epoch 87 finished \tANN training loss 0.484931\n",
      ">> Epoch 88 finished \tANN training loss 0.479694\n",
      ">> Epoch 89 finished \tANN training loss 0.504235\n",
      ">> Epoch 90 finished \tANN training loss 0.650409\n",
      ">> Epoch 91 finished \tANN training loss 0.467515\n",
      ">> Epoch 92 finished \tANN training loss 0.466336\n",
      ">> Epoch 93 finished \tANN training loss 0.475254\n",
      ">> Epoch 94 finished \tANN training loss 0.472950\n",
      ">> Epoch 95 finished \tANN training loss 0.513658\n",
      ">> Epoch 96 finished \tANN training loss 0.484335\n",
      ">> Epoch 97 finished \tANN training loss 0.476890\n",
      ">> Epoch 98 finished \tANN training loss 0.460710\n",
      ">> Epoch 99 finished \tANN training loss 0.479361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 100 finished \tANN training loss 0.468247\n",
      ">> Epoch 101 finished \tANN training loss 0.475698\n",
      ">> Epoch 102 finished \tANN training loss 0.465272\n",
      ">> Epoch 103 finished \tANN training loss 0.480714\n",
      ">> Epoch 104 finished \tANN training loss 0.483871\n",
      ">> Epoch 105 finished \tANN training loss 0.482695\n",
      ">> Epoch 106 finished \tANN training loss 0.471011\n",
      ">> Epoch 107 finished \tANN training loss 0.655381\n",
      ">> Epoch 108 finished \tANN training loss 0.460978\n",
      ">> Epoch 109 finished \tANN training loss 0.520626\n",
      ">> Epoch 110 finished \tANN training loss 0.540664\n",
      ">> Epoch 111 finished \tANN training loss 0.495823\n",
      ">> Epoch 112 finished \tANN training loss 0.465066\n",
      ">> Epoch 113 finished \tANN training loss 0.517478\n",
      ">> Epoch 114 finished \tANN training loss 0.518558\n",
      ">> Epoch 115 finished \tANN training loss 0.544278\n",
      ">> Epoch 116 finished \tANN training loss 0.536436\n",
      ">> Epoch 117 finished \tANN training loss 0.499499\n",
      ">> Epoch 118 finished \tANN training loss 0.457887\n",
      ">> Epoch 119 finished \tANN training loss 0.517949\n",
      ">> Epoch 120 finished \tANN training loss 0.474211\n",
      ">> Epoch 121 finished \tANN training loss 0.473472\n",
      ">> Epoch 122 finished \tANN training loss 0.453038\n",
      ">> Epoch 123 finished \tANN training loss 0.570717\n",
      ">> Epoch 124 finished \tANN training loss 0.534273\n",
      ">> Epoch 125 finished \tANN training loss 0.464087\n",
      ">> Epoch 126 finished \tANN training loss 0.467201\n",
      ">> Epoch 127 finished \tANN training loss 0.494104\n",
      ">> Epoch 128 finished \tANN training loss 0.446274\n",
      ">> Epoch 129 finished \tANN training loss 0.455664\n",
      ">> Epoch 130 finished \tANN training loss 0.484757\n",
      ">> Epoch 131 finished \tANN training loss 0.474717\n",
      ">> Epoch 132 finished \tANN training loss 0.458777\n",
      ">> Epoch 133 finished \tANN training loss 0.491707\n",
      ">> Epoch 134 finished \tANN training loss 0.465750\n",
      ">> Epoch 135 finished \tANN training loss 0.468217\n",
      ">> Epoch 136 finished \tANN training loss 0.437680\n",
      ">> Epoch 137 finished \tANN training loss 0.591155\n",
      ">> Epoch 138 finished \tANN training loss 0.603949\n",
      ">> Epoch 139 finished \tANN training loss 0.441789\n",
      ">> Epoch 140 finished \tANN training loss 0.439341\n",
      ">> Epoch 141 finished \tANN training loss 0.514994\n",
      ">> Epoch 142 finished \tANN training loss 0.497353\n",
      ">> Epoch 143 finished \tANN training loss 0.541455\n",
      ">> Epoch 144 finished \tANN training loss 0.439967\n",
      ">> Epoch 145 finished \tANN training loss 0.454220\n",
      ">> Epoch 146 finished \tANN training loss 0.461963\n",
      ">> Epoch 147 finished \tANN training loss 0.446210\n",
      ">> Epoch 148 finished \tANN training loss 0.453032\n",
      ">> Epoch 149 finished \tANN training loss 0.479431\n",
      ">> Epoch 150 finished \tANN training loss 0.472875\n",
      ">> Epoch 151 finished \tANN training loss 0.479616\n",
      ">> Epoch 152 finished \tANN training loss 0.476120\n",
      ">> Epoch 153 finished \tANN training loss 0.583130\n",
      ">> Epoch 154 finished \tANN training loss 0.442026\n",
      ">> Epoch 155 finished \tANN training loss 0.431914\n",
      ">> Epoch 156 finished \tANN training loss 0.491771\n",
      ">> Epoch 157 finished \tANN training loss 0.596728\n",
      ">> Epoch 158 finished \tANN training loss 0.436193\n",
      ">> Epoch 159 finished \tANN training loss 0.514557\n",
      ">> Epoch 160 finished \tANN training loss 0.435290\n",
      ">> Epoch 161 finished \tANN training loss 0.463586\n",
      ">> Epoch 162 finished \tANN training loss 0.430910\n",
      ">> Epoch 163 finished \tANN training loss 0.559774\n",
      ">> Epoch 164 finished \tANN training loss 0.427648\n",
      ">> Epoch 165 finished \tANN training loss 0.488036\n",
      ">> Epoch 166 finished \tANN training loss 0.452100\n",
      ">> Epoch 167 finished \tANN training loss 0.476446\n",
      ">> Epoch 168 finished \tANN training loss 0.424012\n",
      ">> Epoch 169 finished \tANN training loss 0.449101\n",
      ">> Epoch 170 finished \tANN training loss 0.434540\n",
      ">> Epoch 171 finished \tANN training loss 0.427422\n",
      ">> Epoch 172 finished \tANN training loss 0.468112\n",
      ">> Epoch 173 finished \tANN training loss 0.440230\n",
      ">> Epoch 174 finished \tANN training loss 0.597396\n",
      ">> Epoch 175 finished \tANN training loss 0.450787\n",
      ">> Epoch 176 finished \tANN training loss 0.421614\n",
      ">> Epoch 177 finished \tANN training loss 0.421330\n",
      ">> Epoch 178 finished \tANN training loss 0.440119\n",
      ">> Epoch 179 finished \tANN training loss 0.464270\n",
      ">> Epoch 180 finished \tANN training loss 0.421045\n",
      ">> Epoch 181 finished \tANN training loss 0.459563\n",
      ">> Epoch 182 finished \tANN training loss 0.439610\n",
      ">> Epoch 183 finished \tANN training loss 0.416124\n",
      ">> Epoch 184 finished \tANN training loss 0.469933\n",
      ">> Epoch 185 finished \tANN training loss 0.448063\n",
      ">> Epoch 186 finished \tANN training loss 0.544578\n",
      ">> Epoch 187 finished \tANN training loss 0.507262\n",
      ">> Epoch 188 finished \tANN training loss 0.549916\n",
      ">> Epoch 189 finished \tANN training loss 0.437597\n",
      ">> Epoch 190 finished \tANN training loss 0.428366\n",
      ">> Epoch 191 finished \tANN training loss 0.437155\n",
      ">> Epoch 192 finished \tANN training loss 0.417955\n",
      ">> Epoch 193 finished \tANN training loss 0.494160\n",
      ">> Epoch 194 finished \tANN training loss 0.411744\n",
      ">> Epoch 195 finished \tANN training loss 0.436761\n",
      ">> Epoch 196 finished \tANN training loss 0.423495\n",
      ">> Epoch 197 finished \tANN training loss 0.417196\n",
      ">> Epoch 198 finished \tANN training loss 0.430724\n",
      ">> Epoch 199 finished \tANN training loss 0.418113\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=0,\n",
       "                            idx_to_label_map={0: 0, 1: 1},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1}, learning_rate=0.05,\n",
       "                            n_iter_backprop=200, verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[120, 60],\n",
    "                                         learning_rate_rbm=0.05,\n",
    "                                         learning_rate=0.05,\n",
    "                                         n_epochs_rbm=30,\n",
    "                                         n_iter_backprop=200,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=30)\n",
    "classifier.fit(X_train[:,2:], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ec4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "Y_pred = classifier.predict(X_test[:,2:])\n",
    "Y_pred_p = classifier.predict_proba(X_test[:,2:])[:,1]\n",
    "Y_pred = np.array(Y_pred)\n",
    "Y_test_1 = np.array(Y_test)\n",
    "Y_pred_p = np.array(Y_pred_p)\n",
    "Y_pred_1 = np.reshape(Y_pred,(len(Y_pred),1))\n",
    "Y_test_1 = np.reshape(Y_test_1,(len(Y_test_1),1))\n",
    "Y_pred_p = np.reshape(Y_pred_p,(len(Y_pred_p),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66f257f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Accuracy: 0.737288\n",
      "Done.\n",
      "precision_score: 0.723757\n",
      "Done.\n",
      "recall_score: 0.752874\n",
      "Done.\n",
      "f1_score: 0.738028\n",
      "Done.\n",
      "confusion_matrix: \n",
      "[[130  50]\n",
      " [ 43 131]]\n",
      "Done.\n",
      "cohen_kappa_score: 0.474777\n",
      "Done.\n",
      "hamming_loss: 0.262712\n",
      "Done.\n",
      "auc: 0.831098\n"
     ]
    }
   ],
   "source": [
    "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))\n",
    "print('Done.\\nprecision_score: %f' % precision_score(Y_test, Y_pred))\n",
    "print('Done.\\nrecall_score: %f' % recall_score(Y_test, Y_pred))\n",
    "print('Done.\\nf1_score: %f' % f1_score(Y_test, Y_pred))\n",
    "print('Done.\\nconfusion_matrix: ' )\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print('Done.\\ncohen_kappa_score: %f' % cohen_kappa_score(Y_test, Y_pred))\n",
    "print('Done.\\nhamming_loss: %f' % hamming_loss(Y_test, Y_pred))\n",
    "fpr,tpr,threshold= roc_curve(Y_test,Y_pred_p)\n",
    "print('Done.\\nauc: %f' % auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2fc32f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b451c260c2dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "y_pred = classifier.predict(x_[:,2:])\n",
    "y_pred_p = classifier.predict_proba(x_[:,2:])[:,1]\n",
    "y_pred = np.array(y_pred)\n",
    "y_test_1 = np.array(y_)\n",
    "y_pred_p = np.array(y_pred_p)\n",
    "y_pred_1 = np.reshape(y_pred,(len(y_pred),1))\n",
    "y_test_1 = np.reshape(y_test_1,(len(y_test_1),1))\n",
    "y_pred_p = np.reshape(y_pred_p,(len(y_pred_p),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515e4c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79573648"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape\n",
    "# y_test_1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33cab7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_save_data = np.concatenate((x_,y_test_1),axis=1)\n",
    "mode=0\n",
    "if mode==1:\n",
    "    all_save_data = np.concatenate((all_save_data,y_pred_1),axis=1)\n",
    "else:\n",
    "    all_save_data = np.concatenate((all_save_data,y_pred_p),axis=1)\n",
    "all_save_data_pd = pd.DataFrame(all_save_data,columns=['x','y','road','PGA',\n",
    " 'river','fault','NDVI','lithology','landslide','predict'])\n",
    " # np.savetxt('data.csv',all_save_data,delimiter=',')\n",
    "all_save_data_pd.to_csv('result.csv',index=True)\n",
    "# y_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6135835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Accuracy: 0.720339\n",
      "Done.\n",
      "precision_score: 0.719298\n",
      "Done.\n",
      "recall_score: 0.706897\n",
      "Done.\n",
      "f1_score: 0.713043\n",
      "Done.\n",
      "confusion_matrix: \n",
      "[[132  48]\n",
      " [ 51 123]]\n",
      "Done.\n",
      "cohen_kappa_score: 0.440356\n",
      "Done.\n",
      "hamming_loss: 0.279661\n",
      "Done.\n",
      "auc: 0.815517\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875c1638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f610a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-92c051c2295e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_pred_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#作图\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#数据输入\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#直线\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'False Postive Rate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#x坐标\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fpr' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(np.array(Y_test),Y_pred_p)\n",
    "plt.figure(figsize=(10,8),dpi=300)#作图\n",
    "plt.plot(fpr,tpr)#数据输入\n",
    "plt.plot([0,1],[0,1],linestyle='--')#直线\n",
    "plt.xlabel('False Postive Rate')#x坐标\n",
    "plt.ylabel('True Positive Rate')#y坐标\n",
    "plt.grid()#网格\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,8),dpi=300)#作图\n",
    "plt.plot(precision,recall)#数据输入\n",
    "plt.plot([0,1],[1,0],linestyle='--')#直线\n",
    "plt.xlabel('recall')#x坐标\n",
    "plt.ylabel('precision')#y坐标\n",
    "plt.grid()#网格\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05285c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
