{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb44649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,Dropout,Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f38f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import metrics\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,recall_score,confusion_matrix,f1_score,precision_score\n",
    "from sklearn.metrics import cohen_kappa_score,hamming_loss,precision_recall_curve\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "#分类报告\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "import  tensorflow\n",
    "import  pandas as pd\n",
    "#import eli5\n",
    "#from eli5.sklearn import PermutationImportance\n",
    "#import shap\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print(tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98312723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1336, 13)\n",
      "(1336, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "# digits = load_digits()\n",
    "# X, Y = digits.data, digits.target\n",
    "data = pd.read_csv(r'D:\\python\\TensorflowFiles\\data\\地质灾害易发性\\qiyitrain.csv',sep=',')\n",
    "#data_= pd.read_csv(r'test.csv',sep=',')\n",
    "#导入数据\n",
    "label_index = 8\n",
    "dim  = 8\n",
    "# data[label_index] = data[label_index].map({1:1, 2:-1})\n",
    "data = data.sample(frac=1.0)\n",
    "data = data.reset_index()\n",
    "x = data.drop(['KDE_1'],axis=1)\n",
    "print(x.shape)\n",
    "x = x.drop(['index'],axis=1)\n",
    "x = x.drop(['x大地'],axis=1)\n",
    "x = x.drop(['y大地'],axis=1)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "std=MinMaxScaler((0,1))\n",
    "x=std.fit_transform(x)\n",
    "y = data['KDE_1']\n",
    "y = pd.to_numeric( y, errors='coerce').fillna('0').astype('int32')\n",
    "# Data scaling\n",
    "# X = (X / 16).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58ed3008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9936770, 10)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "data_= pd.read_csv(r'D:\\python\\TensorflowFiles\\data\\地质灾害易发性\\shuju.csv',sep=',')\n",
    "data_.dropna(inplace=True)\n",
    "#导入数据\n",
    "data_ = data_.sample(frac=1.0)\n",
    "data_ = data_.reset_index()\n",
    "x_ = data_.drop(['FID'],axis=1)\n",
    "x_ = x_.drop(['index'],axis=1)\n",
    "res=x_\n",
    "x_ = x_.drop(['x大地'],axis=1)\n",
    "x_ = x_.drop(['y大地'],axis=1)\n",
    "x_ = np.array(x_)\n",
    "print(x_.shape)\n",
    "std=MinMaxScaler((0,1))\n",
    "x_=std.fit_transform(x_)\n",
    "#y_ = data_['KDE_1']\n",
    "#y_ = pd.to_numeric( y_, errors='coerce').fillna('0').astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d993e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.3, random_state=0)#random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f520372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From c:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\python\\TensorflowFiles\\DBN\\dbn\\tensorflow\\models.py:150: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.457747\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.410249\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.357306\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.310640\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.301760\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.277682\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.276459\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.222247\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.226457\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.215930\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.205930\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.215188\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.257949\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.205760\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.229177\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.193885\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.363958\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.351459\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.368748\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.337809\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.325010\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.341120\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.288152\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.264977\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.226502\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.264963\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.254256\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.216766\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.245367\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.226808\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.234090\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.231404\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From D:\\python\\TensorflowFiles\\DBN\\dbn\\tensorflow\\models.py:338: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.185398\n",
      ">> Epoch 1 finished \tANN training loss 1.260160\n",
      ">> Epoch 2 finished \tANN training loss 1.066619\n",
      ">> Epoch 3 finished \tANN training loss 1.089682\n",
      ">> Epoch 4 finished \tANN training loss 0.984706\n",
      ">> Epoch 5 finished \tANN training loss 0.945624\n",
      ">> Epoch 6 finished \tANN training loss 0.932625\n",
      ">> Epoch 7 finished \tANN training loss 0.870445\n",
      ">> Epoch 8 finished \tANN training loss 0.959134\n",
      ">> Epoch 9 finished \tANN training loss 0.878060\n",
      ">> Epoch 10 finished \tANN training loss 0.881518\n",
      ">> Epoch 11 finished \tANN training loss 0.780348\n",
      ">> Epoch 12 finished \tANN training loss 1.196193\n",
      ">> Epoch 13 finished \tANN training loss 0.753991\n",
      ">> Epoch 14 finished \tANN training loss 0.769024\n",
      ">> Epoch 15 finished \tANN training loss 0.770308\n",
      ">> Epoch 16 finished \tANN training loss 0.721727\n",
      ">> Epoch 17 finished \tANN training loss 1.218295\n",
      ">> Epoch 18 finished \tANN training loss 1.132450\n",
      ">> Epoch 19 finished \tANN training loss 0.734287\n",
      ">> Epoch 20 finished \tANN training loss 0.691664\n",
      ">> Epoch 21 finished \tANN training loss 0.684664\n",
      ">> Epoch 22 finished \tANN training loss 0.697855\n",
      ">> Epoch 23 finished \tANN training loss 0.684470\n",
      ">> Epoch 24 finished \tANN training loss 0.891755\n",
      ">> Epoch 25 finished \tANN training loss 0.699078\n",
      ">> Epoch 26 finished \tANN training loss 0.622631\n",
      ">> Epoch 27 finished \tANN training loss 0.819108\n",
      ">> Epoch 28 finished \tANN training loss 0.634240\n",
      ">> Epoch 29 finished \tANN training loss 0.725585\n",
      ">> Epoch 30 finished \tANN training loss 0.621955\n",
      ">> Epoch 31 finished \tANN training loss 0.672223\n",
      ">> Epoch 32 finished \tANN training loss 0.852781\n",
      ">> Epoch 33 finished \tANN training loss 0.675175\n",
      ">> Epoch 34 finished \tANN training loss 0.636099\n",
      ">> Epoch 35 finished \tANN training loss 0.879041\n",
      ">> Epoch 36 finished \tANN training loss 0.579299\n",
      ">> Epoch 37 finished \tANN training loss 0.692600\n",
      ">> Epoch 38 finished \tANN training loss 0.942556\n",
      ">> Epoch 39 finished \tANN training loss 0.580619\n",
      ">> Epoch 40 finished \tANN training loss 0.545897\n",
      ">> Epoch 41 finished \tANN training loss 0.562113\n",
      ">> Epoch 42 finished \tANN training loss 0.564842\n",
      ">> Epoch 43 finished \tANN training loss 0.544109\n",
      ">> Epoch 44 finished \tANN training loss 0.557568\n",
      ">> Epoch 45 finished \tANN training loss 0.512591\n",
      ">> Epoch 46 finished \tANN training loss 0.737961\n",
      ">> Epoch 47 finished \tANN training loss 0.597173\n",
      ">> Epoch 48 finished \tANN training loss 0.582734\n",
      ">> Epoch 49 finished \tANN training loss 0.505549\n",
      ">> Epoch 50 finished \tANN training loss 0.656755\n",
      ">> Epoch 51 finished \tANN training loss 0.529066\n",
      ">> Epoch 52 finished \tANN training loss 0.593229\n",
      ">> Epoch 53 finished \tANN training loss 0.562379\n",
      ">> Epoch 54 finished \tANN training loss 0.542353\n",
      ">> Epoch 55 finished \tANN training loss 0.537463\n",
      ">> Epoch 56 finished \tANN training loss 0.590365\n",
      ">> Epoch 57 finished \tANN training loss 0.481789\n",
      ">> Epoch 58 finished \tANN training loss 0.468505\n",
      ">> Epoch 59 finished \tANN training loss 0.526641\n",
      ">> Epoch 60 finished \tANN training loss 0.546203\n",
      ">> Epoch 61 finished \tANN training loss 0.672980\n",
      ">> Epoch 62 finished \tANN training loss 0.488735\n",
      ">> Epoch 63 finished \tANN training loss 0.614395\n",
      ">> Epoch 64 finished \tANN training loss 0.437791\n",
      ">> Epoch 65 finished \tANN training loss 0.523205\n",
      ">> Epoch 66 finished \tANN training loss 0.772614\n",
      ">> Epoch 67 finished \tANN training loss 0.466857\n",
      ">> Epoch 68 finished \tANN training loss 0.577436\n",
      ">> Epoch 69 finished \tANN training loss 0.623956\n",
      ">> Epoch 70 finished \tANN training loss 0.403450\n",
      ">> Epoch 71 finished \tANN training loss 0.610734\n",
      ">> Epoch 72 finished \tANN training loss 0.857075\n",
      ">> Epoch 73 finished \tANN training loss 0.603701\n",
      ">> Epoch 74 finished \tANN training loss 0.501841\n",
      ">> Epoch 75 finished \tANN training loss 0.429511\n",
      ">> Epoch 76 finished \tANN training loss 0.492290\n",
      ">> Epoch 77 finished \tANN training loss 0.436200\n",
      ">> Epoch 78 finished \tANN training loss 0.392021\n",
      ">> Epoch 79 finished \tANN training loss 0.553371\n",
      ">> Epoch 80 finished \tANN training loss 0.954560\n",
      ">> Epoch 81 finished \tANN training loss 0.422352\n",
      ">> Epoch 82 finished \tANN training loss 0.373961\n",
      ">> Epoch 83 finished \tANN training loss 0.666538\n",
      ">> Epoch 84 finished \tANN training loss 0.442338\n",
      ">> Epoch 85 finished \tANN training loss 0.663073\n",
      ">> Epoch 86 finished \tANN training loss 0.598960\n",
      ">> Epoch 87 finished \tANN training loss 0.374341\n",
      ">> Epoch 88 finished \tANN training loss 0.387045\n",
      ">> Epoch 89 finished \tANN training loss 0.381306\n",
      ">> Epoch 90 finished \tANN training loss 0.379161\n",
      ">> Epoch 91 finished \tANN training loss 0.364053\n",
      ">> Epoch 92 finished \tANN training loss 0.598991\n",
      ">> Epoch 93 finished \tANN training loss 0.356444\n",
      ">> Epoch 94 finished \tANN training loss 0.448737\n",
      ">> Epoch 95 finished \tANN training loss 0.587144\n",
      ">> Epoch 96 finished \tANN training loss 0.496770\n",
      ">> Epoch 97 finished \tANN training loss 0.565713\n",
      ">> Epoch 98 finished \tANN training loss 0.346839\n",
      ">> Epoch 99 finished \tANN training loss 0.372649\n",
      ">> Epoch 100 finished \tANN training loss 0.373679\n",
      ">> Epoch 101 finished \tANN training loss 0.596172\n",
      ">> Epoch 102 finished \tANN training loss 0.599342\n",
      ">> Epoch 103 finished \tANN training loss 0.409542\n",
      ">> Epoch 104 finished \tANN training loss 0.308084\n",
      ">> Epoch 105 finished \tANN training loss 0.677436\n",
      ">> Epoch 106 finished \tANN training loss 0.728092\n",
      ">> Epoch 107 finished \tANN training loss 0.321135\n",
      ">> Epoch 108 finished \tANN training loss 0.541363\n",
      ">> Epoch 109 finished \tANN training loss 0.372370\n",
      ">> Epoch 110 finished \tANN training loss 0.457439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 111 finished \tANN training loss 0.397820\n",
      ">> Epoch 112 finished \tANN training loss 0.471232\n",
      ">> Epoch 113 finished \tANN training loss 0.944304\n",
      ">> Epoch 114 finished \tANN training loss 0.468480\n",
      ">> Epoch 115 finished \tANN training loss 0.398484\n",
      ">> Epoch 116 finished \tANN training loss 0.968459\n",
      ">> Epoch 117 finished \tANN training loss 0.801227\n",
      ">> Epoch 118 finished \tANN training loss 1.583295\n",
      ">> Epoch 119 finished \tANN training loss 0.452989\n",
      ">> Epoch 120 finished \tANN training loss 0.330683\n",
      ">> Epoch 121 finished \tANN training loss 0.302030\n",
      ">> Epoch 122 finished \tANN training loss 0.274218\n",
      ">> Epoch 123 finished \tANN training loss 0.273106\n",
      ">> Epoch 124 finished \tANN training loss 1.014405\n",
      ">> Epoch 125 finished \tANN training loss 0.302554\n",
      ">> Epoch 126 finished \tANN training loss 0.316062\n",
      ">> Epoch 127 finished \tANN training loss 0.281721\n",
      ">> Epoch 128 finished \tANN training loss 0.798365\n",
      ">> Epoch 129 finished \tANN training loss 0.269827\n",
      ">> Epoch 130 finished \tANN training loss 0.584639\n",
      ">> Epoch 131 finished \tANN training loss 0.456591\n",
      ">> Epoch 132 finished \tANN training loss 0.337303\n",
      ">> Epoch 133 finished \tANN training loss 0.701648\n",
      ">> Epoch 134 finished \tANN training loss 0.469677\n",
      ">> Epoch 135 finished \tANN training loss 0.280282\n",
      ">> Epoch 136 finished \tANN training loss 0.285510\n",
      ">> Epoch 137 finished \tANN training loss 0.330299\n",
      ">> Epoch 138 finished \tANN training loss 0.242224\n",
      ">> Epoch 139 finished \tANN training loss 0.404535\n",
      ">> Epoch 140 finished \tANN training loss 0.286649\n",
      ">> Epoch 141 finished \tANN training loss 0.237396\n",
      ">> Epoch 142 finished \tANN training loss 0.256487\n",
      ">> Epoch 143 finished \tANN training loss 0.417272\n",
      ">> Epoch 144 finished \tANN training loss 0.216348\n",
      ">> Epoch 145 finished \tANN training loss 0.250405\n",
      ">> Epoch 146 finished \tANN training loss 0.358516\n",
      ">> Epoch 147 finished \tANN training loss 0.257764\n",
      ">> Epoch 148 finished \tANN training loss 0.245198\n",
      ">> Epoch 149 finished \tANN training loss 0.310060\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=0,\n",
       "                            idx_to_label_map={0: 1, 1: 0, 2: 2, 3: 3},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 1, 1: 0, 2: 2, 3: 3},\n",
       "                            learning_rate=0.15, n_iter_backprop=150,\n",
       "                            verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.3, random_state=0)#random_state=0\n",
    "# Training\n",
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[100, 100],\n",
    "                                         learning_rate_rbm=0.07, \n",
    "                                         learning_rate=0.15,\n",
    "                                         n_epochs_rbm=16,\n",
    "                                         n_iter_backprop=150,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0)\n",
    "classifier.fit(X_train[:,2:], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cdcc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildNet(batch_size1,learning_rate1,learning_rate_rbm1,n_epochs_rbm1,n_iter_backprop1):\n",
    "    classifier = SupervisedDBNClassification(hidden_layers_structure=[100, 100],\n",
    "                                             learning_rate_rbm=learning_rate_rbm1, \n",
    "                                             learning_rate=learning_rate1,\n",
    "                                             n_epochs_rbm=n_epochs_rbm1,\n",
    "                                             n_iter_backprop=n_iter_backprop1,\n",
    "                                             batch_size=batch_size1,\n",
    "                                             activation_function='relu',\n",
    "                                             dropout_p=0)\n",
    "    \n",
    "    #训练\n",
    "    history = classifier.fit(X_train[:,2:], Y_train).history\n",
    "    Y_pred = classifier.predict(X_train[:,2:])\n",
    "    trainAcc = accuracy_score(Y_train, Y_pred)\n",
    "    ##验证集准确率\n",
    "    #valAcc = accuracy_score(Y_val,model.predict(X_val).round(0))\n",
    "    #测试集准确率\n",
    "    testAcc = accuracy_score(Y_test,model.predict(X_test).round(0))\n",
    "    return history,trainAcc,testAcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2db36567",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizeArea = [16,64]\n",
    "learning_rateArea = [0.01,0.2]\n",
    "learning_rate_rbmArea = [0.01,0.2]\n",
    "n_epochs_rbmArea = [20,50]\n",
    "n_iter_backprop = [100,200]\n",
    "\n",
    "##按区间平均值训练一个神经网络\n",
    "#nodeNum = int(np.mean(nodeArea))\n",
    "#p = np.mean(pArea)\n",
    "#defaultNet,defaultHistory,defaultValAcc,defaultTestAcc = buildNet(nodeNum,p)\n",
    "#defaultNet.summary()\n",
    "#print(\"\\n默认网络的 节点数:%d dropout概率:%.2f 验证集准确率:%.4f 测试集准确率:%.4f\"%(nodeNum,p,defaultValAcc,defaultTestAcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45d9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNum = 5#5个需要优化的特征\n",
    "featureArea = [batch_sizeArea,learning_rateArea,learning_rate_rbmArea,n_epochs_rbmArea,n_iter_backprop]#5个特征取值范围\n",
    "featureLimit = [[0,0],[0,0],[0,0],[0,0],[0,0]]#取值范围的开闭  0为闭区间 1为开区间\n",
    "featureType = [int,float,float,int,int]#5个特征的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e6604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7c9fcb047ae6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrainAcc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#开始用粒子群算法迅游\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mpso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalFitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;31m#载入最佳模型和对应的训练历史\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mbestNet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-13383a5e8ca4>\u001b[0m in \u001b[0;36miterate\u001b[1;34m(self, calFitness)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;31m#该粒子的适应度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mfitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalFitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgBest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                 \u001b[1;31m#更新该粒子的自身认知最佳方案\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpBest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfitness\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-7c9fcb047ae6>\u001b[0m in \u001b[0;36mcalFitness\u001b[1;34m(particle, gBest)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'''适应度函数，输入1个粒子的数组和全局最优适应度，返回该粒子对应的适应度'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate_rbm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_epochs_rbm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter_backprop1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparticle\u001b[0m\u001b[1;31m#取出粒子的特征值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainAcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate_rbm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_epochs_rbm1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter_backprop1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#该粒子方案超过全局最优\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalAcc\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mgBest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-c6edcd4428f1>\u001b[0m in \u001b[0;36mbuildNet\u001b[1;34m(batch_size1, learning_rate1, learning_rate_rbm1, n_epochs_rbm1, n_iter_backprop1)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrainAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\TensorflowFiles\\DBN\\dbn\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, pre_train)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \"\"\"\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fine_tuning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\TensorflowFiles\\DBN\\dbn\\models.py\u001b[0m in \u001b[0;36mpre_train\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsupervised_dbn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\TensorflowFiles\\DBN\\dbn\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrbm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrbm_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m             \u001b[0mrbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m             \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\TensorflowFiles\\DBN\\dbn\\tensorflow\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X)\u001b[0m\n",
      "\u001b[1;32mD:\\python\\TensorflowFiles\\DBN\\dbn\\tensorflow\\models.py\u001b[0m in \u001b[0;36m_build_model\u001b[1;34m(self, weights)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[0;32m    245\u001b[0m           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n\u001b[0;32m    246\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m       \u001b[0mrnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[0;32m    813\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m    814\u001b[0m         \u001b[1;34m\"RandomUniform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m                          name=name)\n\u001b[0m\u001b[0;32m    816\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    608\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[0;32m    609\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                        param_name=input_name)\n\u001b[0m\u001b[0;32m    611\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\13390\\miniconda3\\envs\\tensos1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[1;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[0;32m     58\u001b[0m           \u001b[1;34m\"allowed values: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[1;32m---> 60\u001b[1;33m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "#粒子群算法类\n",
    "pso = PSO(featureNum,featureArea,featureLimit,featureType)\n",
    "def calFitness(particle,gBest):\n",
    "    '''适应度函数，输入1个粒子的数组和全局最优适应度，返回该粒子对应的适应度'''\n",
    "    batch_size1,learning_rate1,learning_rate_rbm1,n_epochs_rbm1,n_iter_backprop1 = particle#取出粒子的特征值\n",
    "    history,trainAcc,testAcc = buildNet(batch_size1,learning_rate1,learning_rate_rbm1,n_epochs_rbm1,n_iter_backprop1)\n",
    "    #该粒子方案超过全局最优\n",
    "    if valAcc>gBest:\n",
    "        #保存模型和对应信息\n",
    "        net.save(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/best.h5\")\n",
    "        history = pd.DataFrame(history)\n",
    "        history.to_excel(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/best.xlsx\",index=None)\n",
    "        with open(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/info.json\",\"w\") as f:\n",
    "            f.write(json.dumps({\"trainAcc\":trainAcc,\"testAcc\":testAcc}))\n",
    "    return trainAcc\n",
    "#开始用粒子群算法迅游\n",
    "pso.iterate(calFitness)\n",
    "#载入最佳模型和对应的训练历史\n",
    "bestNet = load_model(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/best.h5\")\n",
    "with open(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/info.json\",\"r\") as f:\n",
    "    info = json.loads(f.read())\n",
    "bestTrainAcc = float(info[\"trainAcc\"])\n",
    "bestTestAcc = float(info[\"testAcc\"])\n",
    "bestHistory = pd.read_excel(\"D:/python/TensorflowFiles/DBN/多分类2/dbn/best.xlsx\")\n",
    "print(\"最优模型的验证集准确率:%.4f 测试集准确率:%.4f\"%(bestTrainAcc,bestTestAcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8ce741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultValAcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2c6a0d32875f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefaultValAcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbestValAcc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefaultTestAcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbestTestAcc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtotal_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultValAcc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.arange(2)\n",
    "a = [defaultValAcc,bestValAcc]\n",
    "b = [defaultTestAcc,bestTestAcc]\n",
    "total_width, n = 0.8, 2\n",
    "width = total_width / n\n",
    "x = x - (total_width - width) / 2\n",
    "ax.bar(x, a,  width=width, label='val',color=\"#00BFFF\")\n",
    "for x1,y1 in zip(x,a):\n",
    "    plt.text(x1,y1+0.01,'%.3f' %y1, ha='center',va='bottom')\n",
    "ax.bar(x + width, b, width=width, label='test',color=\"#FFA500\")\n",
    "for x1,y1 in zip(x,b):\n",
    "    plt.text(x1+width,y1+0.01,'%.3f' %y1, ha='center',va='bottom')\n",
    "ax.legend()\n",
    "ax.set_xticks([0,  1])\n",
    "ax.set_ylim([0,1.2])\n",
    "ax.set_ylabel(\"acc\")\n",
    "ax.set_xticklabels([\"default net\",\"PSO-net\"])\n",
    "fig.savefig(\"Static/对比.png\",dpi=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9ff244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO():\n",
    "    def __init__(self,featureNum,featureArea,featureLimit,featureType,particleNum=5,epochMax=10,c1=2,c2=2):\n",
    "        '''\n",
    "        粒子群算法\n",
    "        :param featureNum: 粒子特征数\n",
    "        :param featureArea: 特征上下限矩阵\n",
    "        :param featureLimit: 特征上下阙界，也是区间的开闭 0为不包含 1为包含\n",
    "        :param featureType: 特征类型 int float\n",
    "        :param particleNum: 粒子个数\n",
    "        :param epochMax: 最大迭代次数\n",
    "        :param c1: 自身认知学习因子\n",
    "        :param c2: 群体认知学习因子\n",
    "        '''\n",
    "        #如上所示\n",
    "        self.featureNum = featureNum\n",
    "        self.featureArea = np.array(featureArea).reshape(featureNum,2)\n",
    "        self.featureLimit = np.array(featureLimit).reshape(featureNum,2)\n",
    "        self.featureType = featureType\n",
    "        self.particleNum = particleNum\n",
    "        self.epochMax = epochMax\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.epoch = 0#已迭代次数\n",
    "        #自身最优适应度记录\n",
    "        self.pBest = [-1e+10 for i in range(particleNum)]\n",
    "        self.pBestArgs = [None for i in range(particleNum)]\n",
    "        #全局最优适应度记录\n",
    "        self.gBest = -1e+10\n",
    "        self.gBestArgs = None\n",
    "        #初始化所有粒子\n",
    "        self.particles = [self.initParticle() for i in range(particleNum)]\n",
    "        #初始化所有粒子的学习速度\n",
    "        self.vs = [np.random.uniform(0,1,size=featureNum) for i in range(particleNum)]\n",
    "        #迭代历史\n",
    "        self.gHistory = {\"特征%d\"%i:[] for i in range(featureNum)}\n",
    "        self.gHistory[\"群内平均\"] = []\n",
    "        self.gHistory[\"全局最优\"] = []\n",
    "\n",
    "    def standardValue(self,value,lowArea,upArea,lowLimit,upLimit,valueType):\n",
    "        '''\n",
    "        规范一个特征值，使其落在区间内\n",
    "        :param value: 特征值\n",
    "        :param lowArea: 下限\n",
    "        :param upArea: 上限\n",
    "        :param lowLimit: 下限开闭区间\n",
    "        :param upLimit: 上限开闭区间\n",
    "        :param valueType: 特征类型\n",
    "        :return: 修正后的值\n",
    "        '''\n",
    "        if value < lowArea:\n",
    "            value = lowArea\n",
    "        if value > upArea:\n",
    "            value = upArea\n",
    "        if valueType is int:\n",
    "            value = np.round(value,0)\n",
    "\n",
    "            #下限为闭区间\n",
    "            if value <= lowArea and lowLimit==0:\n",
    "                value = lowArea + 1\n",
    "            #上限为闭区间\n",
    "            if value >= upArea and upLimit==0:\n",
    "                value = upArea - 1\n",
    "        elif valueType is float:\n",
    "            #下限为闭区间\n",
    "            if value <= lowArea and lowLimit == 0:\n",
    "                value = lowArea + 1e-10\n",
    "            #上限为闭=间\n",
    "            if value >= upArea and upLimit==0:\n",
    "                value = upArea - 1e-10\n",
    "        return value\n",
    "\n",
    "    def initParticle(self):\n",
    "        '''随机初始化1个粒子'''\n",
    "        values = []\n",
    "        #初始化这么多特征数\n",
    "        for i in range(self.featureNum):\n",
    "            #该特征的上下限\n",
    "            lowArea = self.featureArea[i][0]\n",
    "            upArea = self.featureArea[i][1]\n",
    "            #该特征的上下阙界\n",
    "            lowLimit = self.featureLimit[i][0]\n",
    "            upLimit = self.featureLimit[i][1]\n",
    "            #随机值\n",
    "            value = np.random.uniform(0,1) * (upArea-lowArea) + lowArea\n",
    "            value = self.standardValue(value,lowArea,upArea,lowLimit,upLimit,self.featureType[i])\n",
    "            values.append(value)\n",
    "        return values\n",
    "\n",
    "    def iterate(self,calFitness):\n",
    "        '''\n",
    "        开始迭代\n",
    "        :param calFitness:适应度函数 输入为1个粒子的所有特征和全局最佳适应度，输出为适应度\n",
    "        '''\n",
    "        while self.epoch<self.epochMax:\n",
    "            self.epoch += 1\n",
    "            for i,particle in enumerate(self.particles):\n",
    "                #该粒子的适应度\n",
    "                print(particle)\n",
    "                fitness = calFitness(particle,self.gBest)\n",
    "                #更新该粒子的自身认知最佳方案\n",
    "                if self.pBest[i] < fitness:\n",
    "                    self.pBest[i] = fitness\n",
    "                    self.pBestArgs[i] = deepcopy(particle)\n",
    "                #更新全局最佳方案\n",
    "                if self.gBest < fitness:\n",
    "                    self.gBest = fitness\n",
    "                    self.gBestArgs = deepcopy(particle)\n",
    "            #更新粒子\n",
    "            for i, particle in enumerate(self.particles):\n",
    "                #更新速度\n",
    "                self.vs[i] = np.array(self.vs[i]) + self.c1*np.random.uniform(0,1,size=self.featureNum)*(np.array(self.pBestArgs[i])-np.array(self.particles[i])) + self.c2*np.random.uniform(0,1,size=self.featureNum)*(np.array(self.gBestArgs)-np.array(self.particles[i]))\n",
    "                #更新特征值\n",
    "                self.particles[i] = np.array(particle) + self.vs[i]\n",
    "                #规范特征值\n",
    "                values = []\n",
    "                for j in range(self.featureNum):\n",
    "                    #该特征的上下限\n",
    "                    lowArea = self.featureArea[j][0]\n",
    "                    upArea = self.featureArea[j][1]\n",
    "                    #该特征的上下阙界\n",
    "                    lowLimit = self.featureLimit[j][0]\n",
    "                    upLimit = self.featureLimit[j][1]\n",
    "                    #随机值\n",
    "                    value =self.particles[i][j]\n",
    "                    value = self.standardValue(value,lowArea,upArea,lowLimit,upLimit,self.featureType[j])\n",
    "                    values.append(value)\n",
    "                self.particles[i] = values\n",
    "            #保存历史数据\n",
    "            for i in range(self.featureNum):\n",
    "                self.gHistory[\"特征%d\"%i].append(self.gBestArgs[i])\n",
    "            self.gHistory[\"群内平均\"].append(np.mean(self.pBest))\n",
    "            self.gHistory[\"全局最优\"].append(self.gBest)\n",
    "            print(\"PSO epoch:%d/%d 群内平均:%.4f 全局最优:%.4f\"%(self.epoch,self.epochMax,np.mean(self.pBest),self.gBest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf92bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915d837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
